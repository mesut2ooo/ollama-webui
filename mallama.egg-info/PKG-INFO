Metadata-Version: 2.4
Name: mallama
Version: 0.1.3
Summary: Browser UI for Ollama ‚Ä¢ Local LLM Interface ‚Ä¢ Web Chat Client for Local AI Models
Home-page: https://github.com/mesut2ooo/mallama
Author: Masoud Gholypour
Author-email: Masoud Gholypour <masoudgholypour2000@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/mesut2ooo/mallama
Project-URL: Repository, https://github.com/mesut2ooo/mallama.git
Project-URL: Issues, https://github.com/mesut2ooo/mallama/issues
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: flask>=2.0.0
Requires-Dist: requests>=2.28.0
Requires-Dist: werkzeug>=2.0.0
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# Ollama Web UI

A beautiful web interface for Ollama with conversation management and markdown support.

## Features

- üí¨ Chat with Ollama models
- üìù Markdown support with syntax highlighting
- üíæ Save and manage conversations
- ‚öôÔ∏è Adjustable parameters (temperature, top-p, max tokens)
- üìé File upload support
- üé® Beautiful glass-morphism UI
- ‚å®Ô∏è Keyboard shortcuts (Ctrl+C to stop generation)

## Installation

### Via pip
```bash
pip install mallama
mallama --host 0.0.0.0 --port 5000

Via AUR (Arch Linux)
bash

yay -S mallama
# or
paru -S mallama

# Run as a service
systemctl --user enable mallama
systemctl --user start mallama

From source
bash

git clone https://github.com/mesut2ooo/mallama
cd mallama
pip install -e .
mallama

Requirements

    Python 3.8+

    Ollama installed and running locally (http://localhost:11434)

Usage

    Make sure Ollama is running with at least one model pulled

    Start the web UI: mallama

    Open http://localhost:5000 in your browser

    Select a model and start chatting!

Configuration

The application stores conversations and uploads in ~/.mallama/
License

MIT
text


### **tests/__init__.py**
```python
# Test package
